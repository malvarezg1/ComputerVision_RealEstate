{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e1744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os \n",
    "import PIL\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3315bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 8\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dataset sun397: could not find data in C:\\Users\\m.alvarezg1\\tensorflow_datasets. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-da221caa32bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sun397'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[0mas_dataset_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'read_config'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m   \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mas_dataset_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(function, builder, args, kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m           decoders=kwargs.get('decoders', None))\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[1;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m       raise AssertionError(\n\u001b[0m\u001b[0;32m    543\u001b[0m           (\"Dataset %s: could not find data in %s. Please make sure to call \"\n\u001b[0;32m    544\u001b[0m            \u001b[1;34m\"dataset_builder.download_and_prepare(), or pass download=True to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Dataset sun397: could not find data in C:\\Users\\m.alvarezg1\\tensorflow_datasets. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object."
     ]
    }
   ],
   "source": [
    "ds = tfds.load('sun397', split='train', shuffle_files=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb6326fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7449c960b434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df088d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\m.alvarezg1\\tensorflow_datasets\\sun397\\tfds\\4.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5e90069cbc4e14b7cbc9009d156ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1642a9b4d37f4135a6ed60b7a55700c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d7d117990541feb8b9ae72983e891b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64213e76a014cf29c83279e1c5ae143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Image /c/corridor/sun_aspwpqqlcwzfanvl.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/corridor/sun_aznefxvocwpgimko.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/car_interior/backseat/sun_adexwfoqdyhowxpu.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/computer_room/sun_aebgvpgtwoqbfyvl.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/closet/sun_aqnutmwfkypmrnfy.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/closet/sun_absahzamlrylkxyn.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/chicken_coop/outdoor/sun_amaonsnnkskxwmrj.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/chicken_coop/outdoor/sun_abcegmmdbizqkpgh.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/conference_room/sun_aesylgnpqdbvuxoq.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/cockpit/sun_aldrybeuuemusqav.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/cockpit/sun_ajkulpqauavrmxae.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/cockpit/sun_alwmjwxzujyomtvd.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/carrousel/sun_atsgphqympojgxnc.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/carrousel/sun_adsafgyrinnekycc.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/carrousel/sun_auzitjuirwolazns.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/carrousel/sun_anxtrtieimkpmhvk.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_blmmweiumednscuf.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_bzqhuwshtdgakkay.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_byzxhknqrejdajxi.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_bnunxbznqnvgeykx.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_brhmnwzzbkphcvfo.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_bjbmpmqbowmyvxee.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_boagasgfltequmal.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_bgsjjuyqtdleidme.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/church/outdoor/sun_byjkqzybxpjnuofa.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/childs_room/sun_alggivksjwwiklmt.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/control_tower/outdoor/sun_avbcxakrvpomqdgr.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /c/control_tower/outdoor/sun_arohngcbtsvbthho.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /w/wine_cellar/bottle_storage/sun_ahyymswdjejrbhyb.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /w/wine_cellar/bottle_storage/sun_afmzwxkzmxkbamqi.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/art_school/sun_aabogqsjulyvmcse.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/archive/sun_afngadshxudodkct.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/archive/sun_awgsrbljlsvhqjij.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/archive/sun_aegmzltkiwyevpwa.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/airplane_cabin/sun_arqyikigkyfpegug.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/auto_factory/sun_aybymzvbxgvcrwgn.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /a/auto_factory/sun_apfsprenzdnzbhmt.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /m/music_studio/sun_bsntklkmwqgnjrjj.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /v/vineyard/sun_bdxhnbgbnolddswz.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /e/elevator/door/sun_aaudobqlphijkjdv.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /e/elevator_shaft/sun_awaitimkinrjaybl.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /k/kasbah/sun_abxkkoielpavsouu.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /k/kindergarden_classroom/sun_ajtpaahilrqzarri.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /o/oast_house/sun_bqsrrygxyrutgjve.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /r/restaurant_patio/sun_awcbpizjbudjvrhs.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /g/greenhouse/indoor/sun_bkccvyfpwetwjuhk.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /g/greenhouse/indoor/sun_addirvgtxfbndlwf.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fastfood_restaurant/sun_axeniwtesffxqedr.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fastfood_restaurant/sun_anexashcgmxdbmxq.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fastfood_restaurant/sun_aplvzfbmtqtbsvbx.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/formal_garden/sun_bjvrlaeatjufekft.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/firing_range/indoor/sun_affrzvahwjorpalo.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fire_station/sun_bibntbsuunbsdrum.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fire_station/sun_brbskkfgghbfvgkk.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /f/fire_station/sun_bjyapttwilyyuxqm.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /l/living_room/sun_aefoqbeatyufobtx.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /l/landing_deck/sun_acodgoamhgnnbmvr.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /l/laundromat/sun_afrrjykuhhlwiwun.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /t/tower/sun_bccqnzcvqkiwicjt.jpg could not be decoded by OpenCV, falling back to TF\n",
      "WARNING:absl:Image /t/track/outdoor/sun_aophkoiosslinihb.jpg could not be decoded by OpenCV, falling back to TF\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Failed to remove a directory: C:\\Users\\m.alvarezg1\\tensorflow_datasets\\sun397\\tfds\\4.0.0.incompleteSPGW15; Directory not empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Invalid GIF data (size 77719), failed to slurp gif file: Failed to read from given file\n\t [[{{node decode_image_59/DecodeImage}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\u001b[0m in \u001b[0;36m_decode_image\u001b[1;34m(fobj, session, filename)\u001b[0m\n\u001b[0;32m     97\u001b[0m       \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m       \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1394\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Invalid GIF data (size 77719), failed to slurp gif file: Failed to read from given file\n\t [[node decode_image_59/DecodeImage (defined at C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py:97) ]]\n\nOriginal stack trace for 'decode_image_59/DecodeImage':\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-3548eea285c8>\", line 3, in <module>\n    builder.download_and_prepare()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 439, in download_and_prepare\n    self._download_and_prepare(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1137, in _download_and_prepare\n    split_info_futures = [\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1138, in <listcomp>\n    split_builder.submit_split_generation(  # pylint: disable=g-complex-comprehension\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 291, in submit_split_generation\n    return self._build_from_generator(**build_kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 351, in _build_from_generator\n    for key, example in utils.tqdm(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py\", line 248, in __iter__\n    for obj in super(tqdm_notebook, self).__iter__():\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tqdm\\std.py\", line 1178, in __iter__\n    for obj in iterable:\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 291, in _generate_examples\n    image = _process_image_file(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 126, in _process_image_file\n    image = _decode_image(fobj, session, filename=filename)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 97, in _decode_image\n    image = tf.image.decode_image(buf, channels=3)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\", line 3206, in decode_image\n    return gen_image_ops.decode_image(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_image_ops.py\", line 1066, in decode_image\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3557, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36mincomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    303\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[1;32myield\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config)\u001b[0m\n\u001b[0;32m    438\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmock_gfile_pathlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             self._download_and_prepare(\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1137\u001b[1;33m       split_info_futures = [\n\u001b[0m\u001b[0;32m   1138\u001b[0m           split_builder.submit_split_generation(  # pylint: disable=g-complex-comprehension\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1137\u001b[0m       split_info_futures = [\n\u001b[1;32m-> 1138\u001b[1;33m           split_builder.submit_split_generation(  # pylint: disable=g-complex-comprehension\n\u001b[0m\u001b[0;32m   1139\u001b[0m               \u001b[0msplit_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\u001b[0m in \u001b[0;36msubmit_split_generation\u001b[1;34m(self, split_name, generator, path, disable_shuffling)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_from_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbuild_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Otherwise, beam required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\u001b[0m in \u001b[0;36m_build_from_generator\u001b[1;34m(self, split_name, generator, path, disable_shuffling)\u001b[0m\n\u001b[0;32m    350\u001b[0m     )\n\u001b[1;32m--> 351\u001b[1;33m     for key, example in utils.tqdm(\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[1;34m(self, archive, subset_images)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             image = _process_image_file(\n\u001b[0m\u001b[0;32m    292\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\u001b[0m in \u001b[0;36m_process_image_file\u001b[1;34m(fobj, session, filename, quality, target_pixels)\u001b[0m\n\u001b[0;32m    125\u001b[0m   \u001b[1;31m# some encoding options that will make TF crash in general.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m   \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_decode_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m   \u001b[1;31m# Get image height and width.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\u001b[0m in \u001b[0;36m_decode_image\u001b[1;34m(fobj, session, filename)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    101\u001b[0m           f\"{e}. Image {filename} could not be decoded by Tensorflow.\")\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid GIF data (size 77719), failed to slurp gif file: Failed to read from given file\n\t [[node decode_image_59/DecodeImage (defined at C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py:97) ]]\n\nOriginal stack trace for 'decode_image_59/DecodeImage':\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-3548eea285c8>\", line 3, in <module>\n    builder.download_and_prepare()\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 439, in download_and_prepare\n    self._download_and_prepare(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1137, in _download_and_prepare\n    split_info_futures = [\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1138, in <listcomp>\n    split_builder.submit_split_generation(  # pylint: disable=g-complex-comprehension\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 291, in submit_split_generation\n    return self._build_from_generator(**build_kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 351, in _build_from_generator\n    for key, example in utils.tqdm(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py\", line 248, in __iter__\n    for obj in super(tqdm_notebook, self).__iter__():\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tqdm\\std.py\", line 1178, in __iter__\n    for obj in iterable:\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 291, in _generate_examples\n    image = _process_image_file(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 126, in _process_image_file\n    image = _decode_image(fobj, session, filename=filename)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\image_classification\\sun.py\", line 97, in _decode_image\n    image = tf.image.decode_image(buf, channels=3)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\", line 3206, in decode_image\n    return gen_image_ops.decode_image(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_image_ops.py\", line 1066, in decode_image\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3557, in _create_op_internal\n    ret = Operation(\n  File \"C:\\Users\\m.alvarezg1\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2045, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n. Image /t/track/outdoor/sun_aophkoiosslinihb.jpg could not be decoded by Tensorflow.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3548eea285c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sun397'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 1. Create the tfrecord files (no-op if already exists)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# 2. Load the `tf.data.Dataset`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#ds = builder.as_dataset(split='train', shuffle_files=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config)\u001b[0m\n\u001b[0;32m    448\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloaded_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m           \u001b[1;31m# Write DatasetInfo to disk, even if we haven't computed statistics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_to_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_download_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36mincomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    306\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mdelete_recursively_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m   \"\"\"\n\u001b[1;32m--> 677\u001b[1;33m   \u001b[0m_pywrap_file_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to remove a directory: C:\\Users\\m.alvarezg1\\tensorflow_datasets\\sun397\\tfds\\4.0.0.incompleteSPGW15; Directory not empty"
     ]
    }
   ],
   "source": [
    "builder = tfds.builder('sun397')\n",
    "# 1. Create the tfrecord files (no-op if already exists)\n",
    "builder.download_and_prepare()\n",
    "# 2. Load the `tf.data.Dataset`\n",
    "#ds = builder.as_dataset(split='train', shuffle_files=True)\n",
    "#print(ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
